{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GridWorld Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning implementation of GridWorld in [Reinforcement Learning: An Introduction, p72](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) and [Markov Decision Process and Exact Solution Methods](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/slides/Lec2-mdps-exact-methods.pdf). The following descibes the environment and the agent:\n",
        "\n",
        "- The agent (robot) lives in a grid\n",
        "- Walls block the agent’s path\n",
        "- The agent’s actions do not always go as planned:\n",
        "    - 80% of the time, the action North takes the agent North (if there is no wall there)\n",
        "    - 10% of the time, North takes the agent West; 10% East\n",
        "    - If there is a wall in the direction the agent would have been taken, the agent stays put\n",
        "    \n",
        "The first section solves the model-based version of the problem. The second section solves the model-free version of the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-LmJs-yVVlm"
      },
      "source": [
        "## Model-Based Reinforcement Learning\n",
        "\n",
        "Create a `GridWorldMB` class for the model-based problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "csY_-KIN3S7Z"
      },
      "outputs": [],
      "source": [
        "class GridWorldMB():\n",
        "  # actions are the four primary directions\n",
        "  actions = [[0,1], [0,-1], [-1,0], [1,0]]\n",
        "\n",
        "  def __init__(self, size, gamma = 0.9, reward = 0, noise = 0.2):\n",
        "    self.size = size          # dimension of the GridWorld\n",
        "    self.blocked = []         # blocked states\n",
        "    self.terminal = []        # terminal states\n",
        "    self.gamma = gamma        # decay parameter\n",
        "    self.r = reward           # reward per step\n",
        "    self.noise = noise        # noise of action\n",
        "    self.N = self.size[0] * self.size[1]\n",
        "    self.Value = [0 for _ in range(self.N)]\n",
        "    self.Policy = [\"\" for _ in range(self.N)]\n",
        "\n",
        "  # Functions to convert from state to coord and vice versa\n",
        "  def to_state(self, coord):\n",
        "    return self.size[1] * coord[0] + coord[1]\n",
        "  def to_coord(self, state):\n",
        "    return [state//self.size[1], state%self.size[1]]\n",
        "\n",
        "  # Functions to add blocked and terminal coordinates\n",
        "  def add_blocked(self, coord):\n",
        "    self.blocked.append(self.to_state(coord))\n",
        "  def add_terminal(self, coord, val):\n",
        "    self.terminal.append(self.to_state(coord))\n",
        "    self.Value[self.to_state(coord)] = val\n",
        "\n",
        "  # Move function\n",
        "  def move(self, coord, action):\n",
        "    coord_new = coord.copy()\n",
        "\n",
        "    # Move (no restrictions)\n",
        "    coord_new[0] += action[0]\n",
        "    coord_new[1] += action[1]\n",
        "\n",
        "    # Check for walls\n",
        "    coord_new[0] = min(max(0,coord_new[0]),self.size[0]-1)\n",
        "    coord_new[1] = min(max(0,coord_new[1]),self.size[1]-1)\n",
        "\n",
        "    # Check for blocked coords\n",
        "    return coord if self.to_state(coord_new) in self.blocked else coord_new\n",
        "\n",
        "  # Value Iteration\n",
        "  def computeValue(self, max_iter = 1000):\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "      newValue = self.Value.copy()   # Create new copy\n",
        "      maxValue = 0                   # Variable for max across all states\n",
        "\n",
        "      # Check all states\n",
        "      for s in range(self.N):\n",
        "        if s in self.terminal + self.blocked:   # Skip terminal and blocked states\n",
        "          continue\n",
        "\n",
        "        # Current coordinate\n",
        "        coord_curr = self.to_coord(s)\n",
        "        max_val = -1e9\n",
        "\n",
        "        # Run through all actions\n",
        "        for a in self.actions:\n",
        "          val = 0\n",
        "\n",
        "          # Check all possible next states\n",
        "          for a_next in self.actions:\n",
        "            \n",
        "            # (1 - noise) if correct next state\n",
        "            if a_next == a:\n",
        "              prob = 1 - self.noise\n",
        "\n",
        "            # noise / 2 if adjacent next state\n",
        "            elif a_next != [-a[0], -a[1]]:\n",
        "              prob = self.noise / 2\n",
        "            \n",
        "            # 0 if opposite next state\n",
        "            else:\n",
        "              prob = 0\n",
        "\n",
        "            s_next = self.to_state(self.move(coord_curr, a_next))\n",
        "            \n",
        "            # Get sum \n",
        "            val += prob * (self.r + self.gamma * self.Value[s_next])\n",
        "\n",
        "          # Get max across all actions\n",
        "          max_val = max(max_val, val)\n",
        "\n",
        "        # Update max value across all states\n",
        "        maxValue = max(maxValue, abs(max_val - newValue[s]))\n",
        "\n",
        "        # Update current state\n",
        "        newValue[s] = max_val\n",
        "\n",
        "      # Replace variable\n",
        "      self.Value = newValue\n",
        "\n",
        "      # Break if threshold is reached\n",
        "      if maxValue < 1e-20:\n",
        "        break\n",
        "\n",
        "    return self.Value\n",
        "\n",
        "  # Find Optimal Policy\n",
        "  def computePolicy(self):\n",
        "    for s in range(self.N):\n",
        "      if s in self.terminal + self.blocked:   # Skip terminal and blocked states\n",
        "        continue\n",
        "      \n",
        "      max_val = -1e9\n",
        "\n",
        "      for a in self.actions:\n",
        "        if max_val < self.Value[self.to_state(self.move(self.to_coord(s), a))]:\n",
        "          max_val = self.Value[self.to_state(self.move(self.to_coord(s), a))]\n",
        "          self.Policy[s] = a\n",
        "\n",
        "    return self.Policy\n",
        "\n",
        "  # Convert action to arrows\n",
        "  def convert(self, action):\n",
        "    if action == [1,0]:\n",
        "      return \"→\"\n",
        "    if action == [-1,0]:\n",
        "      return \"←\"\n",
        "    if action == [0,1]:\n",
        "      return \"↑\"\n",
        "    if action == [0,-1]:\n",
        "      return \"↓\"\n",
        "    return \".\" \n",
        "  \n",
        "  def result(self):\n",
        "    self.Value = self.computeValue()\n",
        "    self.Policy = self.computePolicy()\n",
        "    \n",
        "    print(\"Value\")\n",
        "    for y in range(self.size[1]-1,-1,-1):\n",
        "      out = \"\"\n",
        "      for x in range(self.size[0]):\n",
        "        out += str(round(self.Value[self.to_state([x,y])],2)) + \"\\t\"\n",
        "      print(out)\n",
        "\n",
        "    print(\"\\nPolicy\")\n",
        "    for y in range(self.size[1]-1,-1,-1):\n",
        "      out = \"\"\n",
        "      for x in range(self.size[0]):\n",
        "        out += self.convert(self.Policy[self.to_state([x,y])]) + \"\\t\"\n",
        "      print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3F9DSmdQd9l"
      },
      "source": [
        "## Model-Free Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Environment\n",
        "\n",
        "Create a `GridWorldMF` class for the model-free problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZWtAPTRea_Nb"
      },
      "outputs": [],
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5OYUtnzunZQ6"
      },
      "outputs": [],
      "source": [
        "class GridWorldMF(Env):\n",
        "  # render parameters\n",
        "  scale = 100\n",
        "  robot = cv2.imread(\"assets/Robot.jpg\") / 255\n",
        "  robot = cv2.resize(robot, (scale, scale))\n",
        "\n",
        "  def __init__(self, size, start = [0,0], noise = 0.2, reward = 0):\n",
        "    self.observation_space = Box(low = 0, high = max(size), shape=(1, 2))\n",
        "    self.action_space = Discrete(4)   # four primary directions\n",
        "    self.state = start                # current state\n",
        "    self.start = start                # starting state\n",
        "    self.size = size                  # dimension of GridWorld\n",
        "    self.noise = noise                # noise of action\n",
        "    self.reward = reward              # reward per step\n",
        "    self.canvas = np.ones((self.size[1] * self.scale, self.size[0] * self.scale, 3)) * 1 # canvas size\n",
        "    self.blocked = []\n",
        "    self.terminal = {}\n",
        "    \n",
        "  # Functions to convert from coord to state\n",
        "  def to_state(self, coord):\n",
        "    return self.size[1] * coord[0] + coord[1]\n",
        "\n",
        "  # Functions to add blocked and terminal coordinates\n",
        "  def add_blocked(self, coord):\n",
        "    y = self.size[1] - coord[1]\n",
        "    x = coord[0]\n",
        "    # render canvas black\n",
        "    self.canvas[y*self.scale-self.scale : y*self.scale, x*self.scale : x*self.scale+self.scale] = [0,0,0]\n",
        "    self.blocked.append(self.to_state(coord))\n",
        "\n",
        "  def add_terminal(self, coord, val):\n",
        "    y = self.size[1] - coord[1]\n",
        "    x = coord[0]\n",
        "    adj = 1 - abs(val) / 10\n",
        "    # render canvas green/red\n",
        "    if val > 0:\n",
        "      self.canvas[y*self.scale-self.scale : y*self.scale, x*self.scale : x*self.scale+self.scale] = [adj,1,adj]\n",
        "    if val < 0:\n",
        "      self.canvas[y*self.scale-self.scale : y*self.scale, x*self.scale : x*self.scale+self.scale] = [adj,adj,1]\n",
        "    self.terminal[self.to_state(coord)] = val\n",
        "      \n",
        "  def step(self, action):\n",
        "    cs = self.state.copy()\n",
        "    \n",
        "    # noise\n",
        "    p = random.random()\n",
        "    if p < self.noise / 2:\n",
        "      action += 1\n",
        "    elif p < self.noise:\n",
        "      action -= 1\n",
        "\n",
        "    if action%4 == 0: # up\n",
        "      cs = [cs[0], cs[1] + 1]\n",
        "    elif action%4 == 1: # right\n",
        "      cs = [cs[0] + 1, cs[1]]\n",
        "    elif action%4 == 2: # down\n",
        "      cs = [cs[0], cs[1] - 1]\n",
        "    elif action%4 == 3: # left\n",
        "      cs = [cs[0] - 1, cs[1]]\n",
        "    \n",
        "    # Check walls and blocked\n",
        "    if not (self.to_state(cs) in self.blocked):\n",
        "      self.state = [min(self.size[0]-1,max(cs[0],0)), min(self.size[1]-1,max(cs[1],0))]\n",
        "\n",
        "    # Calculate reward\n",
        "    if self.to_state(self.state) in self.terminal:\n",
        "      reward = self.terminal[self.to_state(self.state)]\n",
        "    else:\n",
        "      reward = self.reward\n",
        "    \n",
        "    # Check if is done\n",
        "    if self.to_state(self.state) in self.terminal: \n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "\n",
        "    # Set placeholder for info\n",
        "    info = {}\n",
        "    \n",
        "    # Return step information\n",
        "    return self.state, reward, done, info\n",
        "  \n",
        "  # Add grids to render\n",
        "  def add_grids(self, canvas):\n",
        "    for x in range(0, self.size[0]):\n",
        "      for y in range(0, self.size[1]):\n",
        "        canvas[y*self.scale, 0:self.size[0]*self.scale] = [0,0,0]\n",
        "        canvas[0:self.size[1]*self.scale, x*self.scale] = [0,0,0]\n",
        "    return canvas\n",
        "  \n",
        "  # Overlay robot\n",
        "  def overlay_robot(self):\n",
        "    y = self.size[1] - self.state[1]\n",
        "    x = self.state[0]\n",
        "    new_canvas = self.canvas.copy()\n",
        "    for nx in range(x*self.scale, x*self.scale+self.scale):\n",
        "      for ny in range(y*self.scale-self.scale, y*self.scale):\n",
        "        if np.sum(self.robot[ny - y*self.scale, nx - x*self.scale]) > 2.9:\n",
        "          new_canvas[ny, nx] = self.canvas[ny, nx]\n",
        "        else:\n",
        "          new_canvas[ny, nx] = self.robot[ny - y*self.scale, nx - x*self.scale]\n",
        "    return new_canvas\n",
        "\n",
        "  def render(self, mode = \"human\"):\n",
        "    new_canvas = self.overlay_robot()\n",
        "    new_canvas = self.add_grids(new_canvas)\n",
        "    if mode == \"human\":\n",
        "      cv2.imshow(\"GridWorld\", new_canvas)\n",
        "      cv2.waitKey(500) \n",
        "    else:\n",
        "      return new_canvas\n",
        "  \n",
        "  def reset(self):\n",
        "    # Reset position\n",
        "    self.state = self.start\n",
        "    return self.state\n",
        "  \n",
        "  def close(self):\n",
        "    # Close all render window\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo8PA_pCXlCw"
      },
      "source": [
        "### Create Reinforcement Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0CNymuGzSjcS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ta4yGv-4VSo6"
      },
      "outputs": [],
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=states))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(80, activation='relu'))\n",
        "    model.add(Dense(60, activation='relu'))\n",
        "    model.add(Dense(40, activation='relu'))\n",
        "    model.add(Dense(20, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFgVik9RYIHE"
      },
      "source": [
        "### Build Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Tmt_CY9mYKWS"
      },
      "outputs": [],
      "source": [
        "from rl.agents import DQNAgent, SARSAAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jg2GLNAdYT51"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = BoltzmannQPolicy()\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    agent = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
        "    # policy = EpsGreedyQPolicy()\n",
        "    # agent = SARSAAgent(model=model, policy=policy, nb_actions=actions, nb_steps_warmup=10)\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent(env):\n",
        "    states = env.observation_space.shape\n",
        "    actions = env.action_space.n\n",
        "    model = build_model(states, actions)\n",
        "    agent = build_agent(model, actions)\n",
        "    agent.compile(Adam(learning_rate=1e-2), metrics=['mae'])\n",
        "    agent.fit(env, nb_steps=10000, visualize=False, verbose=1)\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example\n",
        "\n",
        "The example below is based on the following environment.\n",
        "\n",
        "<img src=\"assets/Cliff.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model-Based Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK6BPv4w3-dn",
        "outputId": "1adcfea3-52ca-4442-9b5f-6dfc732f6372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value\n",
            "4.48\t5.17\t5.88\t6.68\t7.51\t\n",
            "3.93\t0\t6.03\t7.51\t8.65\t\n",
            "3.45\t0\t1\t0\t10\t\n",
            "2.93\t2.0\t3.31\t5.72\t8.48\t\n",
            "-10\t-10\t-10\t-10\t-10\t\n",
            "\n",
            "Policy\n",
            "→\t→\t→\t↓\t↓\t\n",
            "↑\t.\t→\t→\t↓\t\n",
            "↑\t.\t.\t.\t.\t\n",
            "↑\t→\t→\t→\t↑\t\n",
            ".\t.\t.\t.\t.\t\n"
          ]
        }
      ],
      "source": [
        "g = GridWorldMB([5,5])\n",
        "g.add_blocked([1,2])\n",
        "g.add_blocked([1,3])\n",
        "g.add_blocked([3,2])\n",
        "g.add_terminal([0,0], -10)\n",
        "g.add_terminal([1,0], -10)\n",
        "g.add_terminal([2,0], -10)\n",
        "g.add_terminal([3,0], -10)\n",
        "g.add_terminal([4,0], -10)\n",
        "g.add_terminal([2,2], 1)\n",
        "g.add_terminal([4,2], 10)\n",
        "\n",
        "g.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model-Free Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZnp7NmUpGiY",
        "outputId": "13784383-f256-4ab2-e003-2def653f4ed0"
      },
      "outputs": [],
      "source": [
        "env = GridWorldMF([5,5], start=[0,1], reward=-1)\n",
        "env.add_blocked([1,2])\n",
        "env.add_blocked([1,3])\n",
        "env.add_blocked([3,2])\n",
        "env.add_terminal([0,0], -10)\n",
        "env.add_terminal([1,0], -10)\n",
        "env.add_terminal([2,0], -10)\n",
        "env.add_terminal([3,0], -10)\n",
        "env.add_terminal([4,0], -10)\n",
        "env.add_terminal([2,2], 1)\n",
        "env.add_terminal([4,2], 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2p_2JG5ZFVC",
        "outputId": "52cf33c8-2f67-4e2f-c8b9-5ccf90a747b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 10000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\brian\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 94s 9ms/step - reward: -0.6322\n",
            "done, took 93.757 seconds\n"
          ]
        }
      ],
      "source": [
        "agent = train_agent(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: -1.000, steps: 12\n",
            "Episode 2: reward: 2.000, steps: 9\n",
            "Episode 3: reward: -11.000, steps: 2\n",
            "Episode 4: reward: -3.000, steps: 14\n",
            "Episode 5: reward: 0.000, steps: 11\n",
            "Episode 6: reward: 1.000, steps: 10\n",
            "Episode 7: reward: 1.000, steps: 10\n",
            "Episode 8: reward: -18.000, steps: 29\n",
            "Episode 9: reward: 0.000, steps: 11\n",
            "Episode 10: reward: 1.000, steps: 10\n"
          ]
        }
      ],
      "source": [
        "scores = agent.test(env, nb_episodes=10, visualize=True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combined GridWorld Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorld(Env):\n",
        "  # Action space\n",
        "  actions = [0, 1, 2, 3]  # up, right, down, left\n",
        "  convert = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\"}\n",
        "\n",
        "  # Agent rendering\n",
        "  scale = 100\n",
        "  agent = cv2.resize(cv2.imread(\"assets/Robot.jpg\")/255, (scale, scale))\n",
        "  \n",
        "  def __init__(self, size, start = [0,0], noise = 0.2, reward = 0, gamma = 0.9):\n",
        "    # Open AI Gym parameters\n",
        "    self.action_space = Discrete(4)\n",
        "    self.observation_space = Box(low = 0, high = max(size), shape=(1, 2))\n",
        "    self.length = 1000\n",
        "    \n",
        "    # Environment and State parameters\n",
        "    self.blocked = []\n",
        "    self.terminal = {}\n",
        "    self.size = size\n",
        "    self.N = self.size[0] * self.size[1]\n",
        "    self.state = start\n",
        "    self.start = start\n",
        "\n",
        "    # Other parameters\n",
        "    self.noise = noise\n",
        "    self.reward = reward\n",
        "    self.gamma = gamma\n",
        "\n",
        "    # MDP initialization\n",
        "    self.value = [0 for _ in range(self.N)]\n",
        "    self.policy = [-1 for _ in range(self.N)]\n",
        "    \n",
        "    # Canvas initialization\n",
        "    self.canvas = np.ones((self.size[1] * self.scale, self.size[0] * self.scale, 3)) * 1\n",
        "    self.bumped = -1\n",
        "    self.oops = \"\"\n",
        "\n",
        "  def to_state(self, coord):\n",
        "    return self.size[1] * coord[0] + coord[1]\n",
        "  def to_coord(self, state):\n",
        "    return [state//self.size[1], state%self.size[1]]  \n",
        "\n",
        "  def add_blocked(self, coord):\n",
        "    self.blocked.append(self.to_state(coord))\n",
        "\n",
        "    x = coord[0]; y = self.size[1] - coord[1]\n",
        "    self.canvas[y*self.scale-self.scale : y*self.scale, x*self.scale : x*self.scale+self.scale] = 0\n",
        "  \n",
        "  def add_terminal(self, coord, val):\n",
        "    self.terminal[self.to_state(coord)] = val\n",
        "    self.value[self.to_state(coord)] = val\n",
        "\n",
        "    x = coord[0]; y = self.size[1] - coord[1]\n",
        "    adj = 1 - abs(val) / 10\n",
        "    self.canvas[y*self.scale-self.scale : y*self.scale, x*self.scale : x*self.scale+self.scale] = [adj,1,adj] if val > 0 else [adj,adj,1]\n",
        "      \n",
        "  def step(self, action, noise = None, state = None):\n",
        "    curr_state = self.state if state is None else state\n",
        "    curr_noise = self.noise if noise is None else noise\n",
        "    self.length -= 0 if state is None else 1\n",
        "\n",
        "    # Add noise\n",
        "    p = np.random.uniform()\n",
        "    if p < curr_noise / 2:\n",
        "      action = (action + 1 + 4) % 4\n",
        "      self.oops = \"Oops...\"\n",
        "    elif p < curr_noise:\n",
        "      action = (action - 1 + 4) % 4\n",
        "      self.oops = \"Oops...\"\n",
        "    else:\n",
        "      self.oops = \"\"\n",
        "\n",
        "    # Move to next state\n",
        "    if action == 0: # up\n",
        "      curr_state = [curr_state[0], curr_state[1] + 1]\n",
        "    elif action == 1: # right\n",
        "      curr_state = [curr_state[0] + 1, curr_state[1]]\n",
        "    elif action == 2: # down\n",
        "      curr_state = [curr_state[0], curr_state[1] - 1]\n",
        "    elif action == 3: # left\n",
        "      curr_state = [curr_state[0] - 1, curr_state[1]]\n",
        "    \n",
        "    # Check walls and blocked\n",
        "    if self.to_state(curr_state) not in self.blocked:\n",
        "      walled_state = [min(self.size[0]-1,max(curr_state[0],0)), \n",
        "                    min(self.size[1]-1,max(curr_state[1],0))]\n",
        "      if curr_state != walled_state:\n",
        "        self.bumped = action\n",
        "        curr_state = walled_state\n",
        "      else:\n",
        "        self.bumped = -1\n",
        "    else:\n",
        "      curr_state = self.state if state is None else state\n",
        "      self.bumped = action\n",
        "\n",
        "    # Calculate reward\n",
        "    reward = self.terminal[self.to_state(curr_state)] if self.to_state(curr_state) in self.terminal else self.reward\n",
        "    \n",
        "    # Check if is done\n",
        "    done = self.to_state(curr_state) in self.terminal or self.length == 0\n",
        "\n",
        "    # Set placeholder for info\n",
        "    info = {}\n",
        "    \n",
        "    # Update self.state\n",
        "    if state is None:\n",
        "      self.state = curr_state\n",
        "\n",
        "    # Return step information\n",
        "    return curr_state, reward, done, info\n",
        "  \n",
        "  # Value Iteration\n",
        "  def ValueIteration(self, max_iter = 1000, tolerance = 1e-5, show_iter = False):\n",
        "\n",
        "    for it in range(max_iter):\n",
        "      new_value = self.value.copy()   # Create new copy\n",
        "      delta = 0                       # Max across all states\n",
        "\n",
        "      # Check all states\n",
        "      for state in range(self.N):\n",
        "        if state in list(self.terminal.keys()) + self.blocked:   # Skip terminal and blocked states\n",
        "          continue\n",
        "\n",
        "        # Current coordinate\n",
        "        curr_coord = self.to_coord(state)\n",
        "        max_value = -1e9\n",
        "\n",
        "        # Run through all actions\n",
        "        for action in self.actions:\n",
        "          curr_value = 0\n",
        "\n",
        "          # Check all possible next states\n",
        "          for action_next in self.actions:\n",
        "            \n",
        "            # (1 - noise) if correct next state\n",
        "            if action_next == action:\n",
        "              prob = 1 - self.noise\n",
        "\n",
        "            # noise / 2 if adjacent next state\n",
        "            elif action_next != (action + 2) % 4:\n",
        "              prob = self.noise / 2\n",
        "            \n",
        "            # 0 if opposite next state\n",
        "            else:\n",
        "              prob = 0\n",
        "\n",
        "            state_next = self.to_state(self.step(action_next, noise=0, state=curr_coord)[0])\n",
        "            \n",
        "            # Get sum \n",
        "            curr_value += prob * (self.reward + self.gamma * self.value[state_next])\n",
        "\n",
        "          # Get max across all actions\n",
        "          max_value = max(max_value, curr_value)\n",
        "\n",
        "        # Update max value across all states\n",
        "        delta = max(delta, abs(max_value - new_value[state]))\n",
        "\n",
        "        # Update current state\n",
        "        new_value[state] = max_value\n",
        "\n",
        "      # Replace variable\n",
        "      self.value = new_value\n",
        "\n",
        "      # Break if threshold is reached\n",
        "      if delta < tolerance:\n",
        "        print(\"Final\")\n",
        "        self.OptimalPolicy()\n",
        "        self.show_current(policy=True)\n",
        "        break\n",
        "\n",
        "      # Display iteration\n",
        "      if show_iter:\n",
        "        print(f\"Iteration: {it+1}\")\n",
        "        self.OptimalPolicy()\n",
        "        self.show_current()\n",
        "\n",
        "  # Find Optimal Policy\n",
        "  def OptimalPolicy(self):\n",
        "    for state in range(self.N):\n",
        "      if state in list(self.terminal.keys()) + self.blocked:   # Skip terminal and blocked states\n",
        "        continue\n",
        "      \n",
        "      max_val = -1e9\n",
        "      for action in self.actions:\n",
        "        if max_val < self.value[self.to_state(self.step(action, noise=0, state=self.to_coord(state))[0])]:\n",
        "          max_val = self.value[self.to_state(self.step(action, noise=0, state=self.to_coord(state))[0])]\n",
        "          self.policy[state] = action\n",
        "\n",
        "  # Convert action to arrows\n",
        "  def to_arrow(self, action):\n",
        "    if action in self.convert:\n",
        "      return self.convert[action]\n",
        "    return \".\" \n",
        "  \n",
        "  def show_current(self, value=True, policy=False):    \n",
        "    if value:\n",
        "      print(\"Value Function\")\n",
        "      for y in range(self.size[1]-1,-1,-1):\n",
        "        out = \"\"\n",
        "        for x in range(self.size[0]):\n",
        "          out += str(round(self.value[self.to_state([x,y])],2)) + \"\\t\"\n",
        "        print(out)\n",
        "\n",
        "    if policy:\n",
        "      print(\"\\nCurrent Policy\")\n",
        "      for y in range(self.size[1]-1,-1,-1):\n",
        "        out = \"\"\n",
        "        for x in range(self.size[0]):\n",
        "          out += self.to_arrow(self.policy[self.to_state([x,y])]) + \"\\t\"\n",
        "        print(out)\n",
        "    print('- '*20 + '\\n')\n",
        "\n",
        "  # Add grids to render\n",
        "  def add_grids(self, canvas):\n",
        "    for x in range(0, self.size[0]):\n",
        "      for y in range(0, self.size[1]):\n",
        "        canvas[y*self.scale, 0:self.size[0]*self.scale] = 0\n",
        "        canvas[0:self.size[1]*self.scale, x*self.scale] = 0\n",
        "    return canvas\n",
        "  \n",
        "  # Overlay agent\n",
        "  def overlay_agent(self):\n",
        "    x = self.state[0]; y = self.size[1] - self.state[1]\n",
        "    new_canvas = self.canvas.copy()\n",
        "    for nx in range(x*self.scale, x*self.scale+self.scale):\n",
        "      for ny in range(y*self.scale-self.scale, y*self.scale):\n",
        "        # if np.array_equal(self.agent[ny - y*self.scale, nx - x*self.scale], [1,1,1]):\n",
        "        if np.sum(self.agent[ny - y*self.scale, nx - x*self.scale]) > 2.9:\n",
        "          new_canvas[ny, nx] = self.canvas[ny, nx]\n",
        "        else:\n",
        "          new_canvas[ny, nx] = self.agent[ny - y*self.scale, nx - x*self.scale]  \n",
        "    return new_canvas\n",
        "    \n",
        "  # Other errors\n",
        "  def overlay_errors(self, canvas):\n",
        "    x = self.state[0]; y = self.size[1] - self.state[1]\n",
        "\n",
        "    # Add bumped area\n",
        "    x_range = range(0); y_range = range(0)\n",
        "    if self.bumped == 0:\n",
        "      x_range = range(x*self.scale, x*self.scale+self.scale)\n",
        "      y_range = range(y*self.scale-self.scale, int((y-0.95)*self.scale))\n",
        "    if self.bumped == 1:\n",
        "      x_range = range(int((x+0.95)*self.scale), x*self.scale+self.scale)\n",
        "      y_range = range(y*self.scale-self.scale, y*self.scale)\n",
        "    if self.bumped == 2:\n",
        "      x_range = range(x*self.scale, x*self.scale+self.scale)\n",
        "      y_range = range(int((y+0.95)*self.scale)-self.scale, y*self.scale)\n",
        "    if self.bumped == 3:\n",
        "      x_range = range(x*self.scale, int((x-0.95)*self.scale+self.scale))\n",
        "      y_range = range(y*self.scale-self.scale, y*self.scale)\n",
        "\n",
        "    for nx in x_range:\n",
        "      for ny in y_range: \n",
        "        canvas[ny, nx] = [226/255,43/255,138/255]\n",
        "    \n",
        "    # Add noise \n",
        "    pos = (0, int(self.scale*0.2))\n",
        "    cv2.putText(canvas, self.oops, pos, cv2.FONT_HERSHEY_DUPLEX, fontScale=0.5, color=0)\n",
        "\n",
        "    return canvas\n",
        "    \n",
        "  def render(self, mode = \"human\", wait=800):\n",
        "    new_canvas = self.overlay_agent()\n",
        "    new_canvas = self.add_grids(new_canvas)\n",
        "    new_canvas = self.overlay_errors(new_canvas)\n",
        "    if mode == \"human\":\n",
        "      cv2.imshow(\"GridWorld\", new_canvas)\n",
        "      cv2.waitKey(wait) \n",
        "    else:\n",
        "      return new_canvas\n",
        "  \n",
        "  def reset(self):\n",
        "    self.state = self.start\n",
        "    self.bumped = -1\n",
        "    self.oops = \"\"\n",
        "    return self.state\n",
        "  \n",
        "  def close(self):\n",
        "    cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Reinforcement Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
