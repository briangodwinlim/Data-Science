{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GridWorld Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning implementation of GridWorld in [Reinforcement Learning: An Introduction, p72](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) and [Markov Decision Process and Exact Solution Methods](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/slides/Lec2-mdps-exact-methods.pdf). The following descibes the environment and the agent:\n",
        "\n",
        "- The agent (robot) lives in a grid\n",
        "- Walls block the agent’s path\n",
        "- The agent’s actions do not always go as planned:\n",
        "    - 80% of the time, the action North takes the agent North (if there is no wall there)\n",
        "    - 10% of the time, North takes the agent West; 10% East\n",
        "    - If there is a wall in the direction the agent would have been taken, the agent stays put"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-LmJs-yVVlm"
      },
      "source": [
        "## `GridWorld` Class\n",
        "\n",
        "The `GridWorld` class solves both the model-based and model-free version of the problem.\n",
        "- Normal states are represented as coordinates *i.e.* $(x,y,0)$.\n",
        "- Goal states are flagged by its third coordinate *i.e.* $(x,y,1)$.\n",
        "- `GridWorld` has a `step` method to let the agent perform an action. \n",
        "- `GridWorld` has a `render` method to render the environment and agent.\n",
        "- For the model-based version of the problem, it can perform `ValueIteration` and `PolicyIteration` to determine the optimal policy.\n",
        "- For the model-free version of the problem, it uses `keras-rl`, `tensorflow`, and `gym` to determine the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "csY_-KIN3S7Z"
      },
      "outputs": [],
      "source": [
        "class GridWorld(Env):\n",
        "  # Action space\n",
        "  actions = [0, 1, 2, 3]  # up, right, down, left\n",
        "  convert = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\", \".\": \".\"}\n",
        "\n",
        "  # Agent rendering\n",
        "  scale = 100\n",
        "  agent = cv2.resize(cv2.imread(\"assets/Robot.jpg\")/255, (scale, scale))\n",
        "  \n",
        "  def __init__(self, size, start = [0,0,0], noise = 0.2, reward = 0, gamma = 0.9):\n",
        "    # Open AI Gym parameters\n",
        "    self.action_space = Discrete(4)\n",
        "    self.observation_space = Box(low = 0, high = max(size), shape=(1, 3))\n",
        "    self.length = 1000\n",
        "    \n",
        "    # Environment and State parameters\n",
        "    self.blocked = []\n",
        "    self.nterminal = []\n",
        "    self.size = size\n",
        "    self.N = self.size[0] * self.size[1]\n",
        "    self.state = start\n",
        "    self.start = start\n",
        "\n",
        "    # Other parameters\n",
        "    self.noise = noise\n",
        "    self.reward_map = {i:reward for i in range(self.N)}\n",
        "    self.gamma = gamma\n",
        "\n",
        "    # MDP initialization\n",
        "    self.value = [0 for _ in range(2*self.N)]\n",
        "    self.policy = [['.'] for _ in range(self.N)]\n",
        "    self.Q = [[0 for _ in range(len(self.actions))] for _ in range(self.N)]\n",
        "    \n",
        "    # Canvas initialization\n",
        "    self.canvas = np.ones((self.size[1] * self.scale, self.size[0] * self.scale, 3)) * 1\n",
        "    self.bumped = -1\n",
        "    self.oops = \"\"\n",
        "\n",
        "  # Converting from integers to coordinate systems\n",
        "  def to_state(self, coord):\n",
        "    return self.size[0] * coord[1] + coord[0] + self.N * coord[2] \n",
        "  def to_coord(self, state):\n",
        "    return [(state%self.N)%self.size[0], (state%self.N)//self.size[0], state//self.N]\n",
        "  \n",
        "  # Add a reward for a specific state\n",
        "  def add_reward(self, coord, val):\n",
        "    self.reward_map[self.to_state(coord)] = val\n",
        "\n",
        "  # Add blocked states\n",
        "  def add_blocked(self, coord):\n",
        "    self.blocked.append(self.to_state(coord))\n",
        "\n",
        "    x = coord[0]; y = self.size[1] - coord[1]\n",
        "    self.canvas[y*self.scale-self.scale : y*self.scale, x*self.scale : x*self.scale+self.scale] = 0\n",
        "  \n",
        "  # Add near terminal states (penultimate states from goal states)\n",
        "  def add_nterminal(self, coord, val):\n",
        "    self.nterminal.append(self.to_state(coord))\n",
        "    goal_coord = coord.copy()\n",
        "    goal_coord[2] = 1\n",
        "    self.add_reward(goal_coord, val)\n",
        "\n",
        "    x = coord[0]; y = self.size[1] - coord[1]\n",
        "    adj = 1 - abs(val) / 10\n",
        "    self.canvas[y*self.scale-self.scale : y*self.scale, x*self.scale : x*self.scale+self.scale] = [adj,1,adj] if val > 0 else [adj,adj,1]\n",
        "      \n",
        "  # Method to let the agent perform an action\n",
        "  def step(self, action, noise = None, state = None):\n",
        "    curr_state = self.state if state is None else state.copy()\n",
        "    curr_noise = self.noise if noise is None else noise\n",
        "    self.length -= 0 if state is None else 1\n",
        "\n",
        "    # Check terminal states\n",
        "    if curr_state[2] == 1:\n",
        "      return curr_state, self.reward_map[self.to_state(curr_state)], True, {}\n",
        "    \n",
        "    # Check near terminal states\n",
        "    if self.to_state(curr_state) in self.nterminal:\n",
        "      curr_state[2] = 1\n",
        "      return curr_state, self.reward_map[self.to_state(curr_state)], True, {}\n",
        "\n",
        "    # Add noise\n",
        "    p = np.random.uniform()\n",
        "    if p < curr_noise / 2:\n",
        "      action = (action + 1 + 4) % 4\n",
        "      self.oops = \"Oops...\"\n",
        "    elif p < curr_noise:\n",
        "      action = (action - 1 + 4) % 4\n",
        "      self.oops = \"Oops...\"\n",
        "    else:\n",
        "      self.oops = \"\"\n",
        "\n",
        "    # Move to next state\n",
        "    if action == 0:   # up\n",
        "      curr_state = [curr_state[0], curr_state[1] + 1, curr_state[2]]\n",
        "    elif action == 1: # right\n",
        "      curr_state = [curr_state[0] + 1, curr_state[1], curr_state[2]]\n",
        "    elif action == 2: # down\n",
        "      curr_state = [curr_state[0], curr_state[1] - 1, curr_state[2]]\n",
        "    elif action == 3: # left\n",
        "      curr_state = [curr_state[0] - 1, curr_state[1], curr_state[2]]\n",
        "    \n",
        "    # Check walls and blocked\n",
        "    if self.to_state(curr_state) not in self.blocked:\n",
        "      walled_state = [min(self.size[0]-1,max(curr_state[0],0)), \n",
        "                    min(self.size[1]-1,max(curr_state[1],0)),\n",
        "                    curr_state[2]]\n",
        "      if curr_state != walled_state:\n",
        "        self.bumped = action\n",
        "        curr_state = walled_state\n",
        "      else:\n",
        "        self.bumped = -1\n",
        "    else:\n",
        "      curr_state = self.state if state is None else state\n",
        "      self.bumped = action\n",
        "\n",
        "    # Calculate reward\n",
        "    reward = self.reward_map[self.to_state(curr_state)] \n",
        "\n",
        "    # Check if is done\n",
        "    done = self.length == 0\n",
        "\n",
        "    # Set placeholder for info\n",
        "    info = {}\n",
        "    \n",
        "    # Update self.state\n",
        "    if state is None:\n",
        "      self.state = curr_state\n",
        "\n",
        "    # Return step information\n",
        "    return curr_state, reward, done, info\n",
        "  \n",
        "  # Value Iteration to determine the optimal policy\n",
        "  def ValueIteration(self, max_iter = 1000, tolerance = 1e-5, show_iter = False):\n",
        "\n",
        "    for it in range(max_iter):\n",
        "      new_value = self.value.copy()   # Create new copy\n",
        "      delta = 0                       # Max across all states\n",
        "\n",
        "      # Check all states\n",
        "      for state in range(self.N):\n",
        "        if state in self.blocked:   # Skip blocked states\n",
        "          continue\n",
        "\n",
        "        # Current coordinate\n",
        "        curr_coord = self.to_coord(state)\n",
        "        max_value = -1e9\n",
        "\n",
        "        # Run through all actions\n",
        "        for action in self.actions:\n",
        "          self.Q[state][action] = 0\n",
        "\n",
        "          # Check all possible next states\n",
        "          for action_next in self.actions:\n",
        "            \n",
        "            # (1 - noise) if correct next state\n",
        "            if action_next == action:\n",
        "              prob = 1 - self.noise\n",
        "\n",
        "            # noise / 2 if adjacent next state\n",
        "            elif action_next != (action + 2) % 4:\n",
        "              prob = self.noise / 2\n",
        "            \n",
        "            # 0 if opposite next state\n",
        "            else:\n",
        "              prob = 0\n",
        "\n",
        "            state_next, reward, done, info = self.step(action_next, noise=0, state=curr_coord)\n",
        "            state_next = self.to_state(state_next)\n",
        "            \n",
        "            # Get sum \n",
        "            self.Q[state][action] += prob * (reward + self.gamma * self.value[state_next])\n",
        "            \n",
        "          # Get max across all actions\n",
        "          max_value = max(max_value, self.Q[state][action])\n",
        "\n",
        "        # Update max value across all states\n",
        "        delta = max(delta, abs(max_value - new_value[state]))\n",
        "\n",
        "        # Update current state\n",
        "        new_value[state] = max_value\n",
        "\n",
        "      # Replace variable\n",
        "      self.value = new_value\n",
        "\n",
        "      # Break if threshold is reached\n",
        "      if delta < tolerance or it == max_iter - 1:\n",
        "        print(\"Final\")\n",
        "        self.OptimalPolicy()\n",
        "        self.show_current(policy=True)\n",
        "        break\n",
        "\n",
        "      # Display iteration\n",
        "      if show_iter:\n",
        "        print(f\"Iteration: {it+1}\")\n",
        "        self.OptimalPolicy()\n",
        "        self.show_current()\n",
        "\n",
        "  # Find Optimal Policy\n",
        "  def OptimalPolicy(self):\n",
        "    for state in range(self.N):\n",
        "      if state in self.nterminal + self.blocked:   # Skip nterminal and blocked states\n",
        "        continue\n",
        "      self.policy[state] = list(np.flatnonzero(self.Q[state] == np.max(self.Q[state])))\n",
        "\n",
        "  # Policy Evaluation\n",
        "  def PolicyEvaluation(self, max_iter = 100, tolerance = 1e-5):\n",
        "    for it in range(max_iter):\n",
        "      new_value = self.value.copy()     # Create new copy\n",
        "      delta = 0                         # Max across all states\n",
        "\n",
        "      # Check all states\n",
        "      for state in range(self.N):\n",
        "        curr_value = 0\n",
        "        if state in self.blocked:   # Skip blocked states\n",
        "          continue\n",
        "\n",
        "        # Current coordinate\n",
        "        curr_coord = self.to_coord(state)\n",
        "\n",
        "        # Action from policy\n",
        "        action = np.random.choice(self.policy[state], size=1)[0]\n",
        "        action = np.random.randint(0,4) if isinstance(action, str) else action\n",
        "\n",
        "        # Check all possible next states\n",
        "        for action_next in self.actions:\n",
        "          \n",
        "          # (1 - noise) if correct next state\n",
        "          if action_next == action:\n",
        "            prob = 1 - self.noise\n",
        "\n",
        "          # noise / 2 if adjacent next state\n",
        "          elif action_next != (action + 2) % 4:\n",
        "            prob = self.noise / 2\n",
        "          \n",
        "          # 0 if opposite next state\n",
        "          else:\n",
        "            prob = 0\n",
        "\n",
        "          state_next, reward, done, info = self.step(action_next, noise=0, state=curr_coord)\n",
        "          state_next = self.to_state(state_next)\n",
        "          \n",
        "          # Get sum \n",
        "          curr_value += prob * (reward + self.gamma * self.value[state_next])\n",
        "          \n",
        "        # Update max value across all states\n",
        "        delta = max(delta, abs(curr_value - new_value[state]))\n",
        "\n",
        "        # Update current state\n",
        "        new_value[state] = curr_value\n",
        "\n",
        "      # Replace variable\n",
        "      self.value = new_value\n",
        "\n",
        "      # Return if threshold is reached\n",
        "      if delta < tolerance or it == max_iter - 1:\n",
        "        break\n",
        "\n",
        "  # Policy Improvement\n",
        "  def PolicyImprovement(self):\n",
        "    self.policy = [['.'] for _ in range(self.N)]\n",
        "    self.Q = [[0 for _ in range(len(self.actions))] for _ in range(self.N)]\n",
        "\n",
        "    # Check all states\n",
        "    for state in range(self.N):\n",
        "      if state in self.blocked + self.nterminal:   # Skip blocked states\n",
        "        continue\n",
        "\n",
        "      # Current coordinate\n",
        "      curr_coord = self.to_coord(state)\n",
        "\n",
        "      # Run through all actions\n",
        "      for action in self.actions:\n",
        "\n",
        "        # Check all possible next states\n",
        "        for action_next in self.actions:\n",
        "          \n",
        "          # (1 - noise) if correct next state\n",
        "          if action_next == action:\n",
        "            prob = 1 - self.noise\n",
        "\n",
        "          # noise / 2 if adjacent next state\n",
        "          elif action_next != (action + 2) % 4:\n",
        "            prob = self.noise / 2\n",
        "          \n",
        "          # 0 if opposite next state\n",
        "          else:\n",
        "            prob = 0\n",
        "\n",
        "          state_next, reward, done, info = self.step(action_next, noise=0, state=curr_coord)\n",
        "          state_next = self.to_state(state_next)\n",
        "          \n",
        "          # Get sum \n",
        "          self.Q[state][action] += prob * (reward + self.gamma * self.value[state_next])\n",
        "\n",
        "      # Get optimal policy\n",
        "      self.policy[state] = list(np.flatnonzero(self.Q[state] == np.max(self.Q[state])))\n",
        "    \n",
        "  # Policy Iteration to determine optimal policy\n",
        "  def PolicyIteration(self, max_iter = 100, tolerance = 1e-5, show_iter = False):\n",
        "    self.value = [0 for _ in range(2*self.N)]\n",
        "    self.policy = [['.'] for _ in range(self.N)]\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        old_policy = self.policy.copy()\n",
        "        self.PolicyEvaluation(max_iter, tolerance)\n",
        "        self.PolicyImprovement()\n",
        "        \n",
        "        # Break if policy did not change\n",
        "        if self.policy == old_policy or it == max_iter - 1:\n",
        "          print(\"Final\")\n",
        "          self.show_current(policy=True)\n",
        "          break\n",
        "\n",
        "        # Display iteration\n",
        "        if show_iter:\n",
        "          print(f\"Iteration: {it+1}\")\n",
        "          self.show_current()\n",
        "\n",
        "  # Convert action to arrows\n",
        "  def to_arrow(self, action):\n",
        "    if action in self.convert:\n",
        "      return self.convert[action]\n",
        "    return \".\" \n",
        "  \n",
        "  # Print the current value function and/or policy\n",
        "  def show_current(self, value=True, policy=False):    \n",
        "    if value:\n",
        "      print(\"Value Function\")\n",
        "      for y in range(self.size[1]-1,-1,-1):\n",
        "        out = \"\"\n",
        "        for x in range(self.size[0]):\n",
        "          out += str(round(self.value[self.to_state([x,y,0])],2)) + \"\\t\"\n",
        "        print(out)\n",
        "\n",
        "    if policy:\n",
        "      print(\"\\nCurrent Policy\")\n",
        "      for y in range(self.size[1]-1,-1,-1):\n",
        "        out = \"\"\n",
        "        for x in range(self.size[0]):\n",
        "          out += self.to_arrow(self.policy[self.to_state([x,y,0])][0]) + \"\\t\"\n",
        "        print(out)\n",
        "    print('- '*20 + '\\n')\n",
        "\n",
        "  # Add grids to render\n",
        "  def add_grids(self, canvas):\n",
        "    for x in range(0, self.size[0]):\n",
        "      for y in range(0, self.size[1]):\n",
        "        canvas[y*self.scale, 0:self.size[0]*self.scale] = 0\n",
        "        canvas[0:self.size[1]*self.scale, x*self.scale] = 0\n",
        "    return canvas\n",
        "  \n",
        "  # Overlay agent\n",
        "  def overlay_agent(self):\n",
        "    x = self.state[0]; y = self.size[1] - self.state[1]\n",
        "    new_canvas = self.canvas.copy()\n",
        "    if self.state[2] == 1:\n",
        "      self.oops = \"Goal State reached\"\n",
        "    for nx in range(x*self.scale, x*self.scale+self.scale):\n",
        "      for ny in range(y*self.scale-self.scale, y*self.scale):\n",
        "        if np.sum(self.agent[ny - y*self.scale, nx - x*self.scale]) > 2.9:\n",
        "          new_canvas[ny, nx] = self.canvas[ny, nx]\n",
        "        else:\n",
        "          new_canvas[ny, nx] = self.agent[ny - y*self.scale, nx - x*self.scale]\n",
        "    return new_canvas\n",
        "    \n",
        "  # Other errors\n",
        "  def overlay_errors(self, canvas):\n",
        "    x = self.state[0]; y = self.size[1] - self.state[1]\n",
        "\n",
        "    # Add bumped area\n",
        "    x_range = range(0); y_range = range(0)\n",
        "    if self.bumped == 0:\n",
        "      x_range = range(x*self.scale, x*self.scale+self.scale)\n",
        "      y_range = range(y*self.scale-self.scale, int((y-0.95)*self.scale))\n",
        "    if self.bumped == 1:\n",
        "      x_range = range(int((x+0.95)*self.scale), x*self.scale+self.scale)\n",
        "      y_range = range(y*self.scale-self.scale, y*self.scale)\n",
        "    if self.bumped == 2:\n",
        "      x_range = range(x*self.scale, x*self.scale+self.scale)\n",
        "      y_range = range(int((y+0.95)*self.scale)-self.scale, y*self.scale)\n",
        "    if self.bumped == 3:\n",
        "      x_range = range(x*self.scale, int((x-0.95)*self.scale+self.scale))\n",
        "      y_range = range(y*self.scale-self.scale, y*self.scale)\n",
        "\n",
        "    for nx in x_range:\n",
        "      for ny in y_range: \n",
        "        canvas[ny, nx] = [226/255,43/255,138/255]\n",
        "    \n",
        "    # Add noise \n",
        "    pos = (0, int(self.scale*0.2))\n",
        "    cv2.putText(canvas, self.oops, pos, cv2.FONT_HERSHEY_DUPLEX, fontScale=0.5, color=0)\n",
        "\n",
        "    return canvas\n",
        "    \n",
        "  # Render the environment and agent\n",
        "  def render(self, mode = \"human\", wait=800):\n",
        "    new_canvas = self.overlay_agent()\n",
        "    new_canvas = self.add_grids(new_canvas)\n",
        "    new_canvas = self.overlay_errors(new_canvas)\n",
        "    if mode == \"human\":\n",
        "      cv2.imshow(\"GridWorld\", new_canvas)\n",
        "      cv2.waitKey(wait) \n",
        "    else:\n",
        "      return new_canvas\n",
        "  \n",
        "  # Reset the environment and agent\n",
        "  def reset(self):\n",
        "    self.state = self.start\n",
        "    self.bumped = -1\n",
        "    self.oops = \"\"\n",
        "    self.value = [0 for _ in range(2*self.N)]\n",
        "    self.policy = [['.'] for _ in range(self.N)]\n",
        "    return self.state\n",
        "  \n",
        "  # Close all render windows\n",
        "  def close(self):\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3F9DSmdQd9l"
      },
      "source": [
        "## Model-Free Reinforcement Learning\n",
        "\n",
        "The functions below trains an agent in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo8PA_pCXlCw"
      },
      "source": [
        "### Create Reinforcement Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0CNymuGzSjcS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ta4yGv-4VSo6"
      },
      "outputs": [],
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=states))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(80, activation='relu'))\n",
        "    model.add(Dense(60, activation='relu'))\n",
        "    model.add(Dense(40, activation='relu'))\n",
        "    model.add(Dense(20, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFgVik9RYIHE"
      },
      "source": [
        "### Build Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tmt_CY9mYKWS"
      },
      "outputs": [],
      "source": [
        "from rl.agents import DQNAgent, SARSAAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jg2GLNAdYT51"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions):\n",
        "    memory = SequentialMemory(limit=50000, window_length=1)\n",
        "    policy = BoltzmannQPolicy()\n",
        "    agent = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
        "    # policy = EpsGreedyQPolicy()\n",
        "    # agent = SARSAAgent(model=model, policy=policy, nb_actions=actions, nb_steps_warmup=10)\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent(env):\n",
        "    states = env.observation_space.shape\n",
        "    actions = env.action_space.n\n",
        "    model = build_model(states, actions)\n",
        "    agent = build_agent(model, actions)\n",
        "    agent.compile(Adam(learning_rate=1e-2), metrics=['mae'])\n",
        "    agent.fit(env, nb_steps=10000, visualize=False, verbose=1)\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example\n",
        "\n",
        "The example below is based on the following environment.\n",
        "\n",
        "<img src=\"assets/Cliff.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld([5,5], start=[0,1,0], reward=0)\n",
        "env.add_blocked([1,2,0])\n",
        "env.add_blocked([1,3,0])\n",
        "env.add_blocked([3,2,0])\n",
        "env.add_nterminal([0,0,0], -10)\n",
        "env.add_nterminal([1,0,0], -10)\n",
        "env.add_nterminal([2,0,0], -10)\n",
        "env.add_nterminal([3,0,0], -10)\n",
        "env.add_nterminal([4,0,0], -10)\n",
        "env.add_nterminal([2,2,0], 1)\n",
        "env.add_nterminal([4,2,0], 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample Simulation\n",
        "\n",
        "The cell below accepts a string containing the direction of the agent and renders a sample simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 1\n"
          ]
        }
      ],
      "source": [
        "to_int = {'u': 0, 'r': 1, 'd': 2, 'l': 3}\n",
        "\n",
        "n_state = env.reset()\n",
        "done = False\n",
        "score = 0\n",
        "\n",
        "actions = input('Input Move List: ')\n",
        "# Example: uuurrrrddd\n",
        "\n",
        "env.render()\n",
        "for action in actions:\n",
        "    if action in to_int:\n",
        "        action = to_int[action]\n",
        "    else:\n",
        "        print('Please input a valid action')\n",
        "        env.close() \n",
        "        break\n",
        "    n_state, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "    env.render()\n",
        "    if done: \n",
        "        break\n",
        "print(f'Score: {score}')\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model-Based Case\n",
        "\n",
        "This subsection presents the optimal policy using `ValueIteration` and `PolicyIteration`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK6BPv4w3-dn",
        "outputId": "1adcfea3-52ca-4442-9b5f-6dfc732f6372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final\n",
            "Value Function\n",
            "4.48\t5.17\t5.88\t6.68\t7.51\t\n",
            "3.93\t0\t6.03\t7.51\t8.65\t\n",
            "3.45\t0\t1.0\t0\t10.0\t\n",
            "2.93\t2.0\t3.31\t5.72\t8.48\t\n",
            "-10.0\t-10.0\t-10.0\t-10.0\t-10.0\t\n",
            "\n",
            "Current Policy\n",
            "→\t→\t→\t→\t↓\t\n",
            "↑\t.\t→\t→\t↓\t\n",
            "↑\t.\t.\t.\t.\t\n",
            "↑\t↑\t→\t→\t↑\t\n",
            ".\t.\t.\t.\t.\t\n",
            "- - - - - - - - - - - - - - - - - - - - \n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.ValueIteration(show_iter=False, max_iter=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final\n",
            "Value Function\n",
            "4.48\t5.17\t5.88\t6.68\t7.51\t\n",
            "3.93\t0\t6.03\t7.51\t8.65\t\n",
            "3.45\t0\t1.0\t0\t10.0\t\n",
            "2.93\t2.0\t3.31\t5.72\t8.48\t\n",
            "-10.0\t-10.0\t-10.0\t-10.0\t-10.0\t\n",
            "\n",
            "Current Policy\n",
            "→\t→\t→\t→\t↓\t\n",
            "↑\t.\t→\t→\t↓\t\n",
            "↑\t.\t.\t.\t.\t\n",
            "↑\t↑\t→\t→\t↑\t\n",
            ".\t.\t.\t.\t.\t\n",
            "- - - - - - - - - - - - - - - - - - - - \n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.PolicyIteration(show_iter=False, max_iter=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model-Free Case\n",
        "\n",
        "This subsection determines the optimal policy using the previously defined functions.\n",
        "\n",
        "Note: Weird behavior can be observed in the model-free case. This might be due to lack of iterations and/or the type of policy/agent used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 10000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 187s 19ms/step - reward: -0.0141\n",
            "done, took 187.317 seconds\n"
          ]
        }
      ],
      "source": [
        "agent = train_agent(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 1 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 46\n"
          ]
        }
      ],
      "source": [
        "scores = agent.test(env, nb_episodes=1, visualize=True)\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Reinforcement Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
